# ğŸ Hive Backend

A Reddit-like application backend with an intelligent verification system powered by web scraping and LangChain RAG pipeline.

## ğŸ“– Project Description

Hive is a Reddit-like social media platform that includes an advanced content verification system. The backend is built with FastAPI and features:

-   **Content Verification**: Automated fact-checking using web scraping and AI
-   **RAG Pipeline**: LangChain-powered retrieval-augmented generation for content analysis
-   **Modern API**: FastAPI-based RESTful API with automatic documentation
-   **Scalable Architecture**: Modular design for easy maintenance and scaling

## ğŸ“‚ Folder Structure

```
hive-backend/
â”œâ”€â”€ app/                    # Main application package
â”‚   â”œâ”€â”€ main.py            # FastAPI entry point with health routes
â”‚   â”œâ”€â”€ api/               # API routes and endpoints
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py      # TODO: API route implementations
â”‚   â”œâ”€â”€ core/              # Configuration and settings
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py      # TODO: App configuration
â”‚   â”œâ”€â”€ models/            # Data models and schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py     # TODO: Pydantic models
â”‚   â”œâ”€â”€ services/          # Business logic layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ post_service.py        # TODO: Post operations
â”‚   â”‚   â””â”€â”€ verification_service.py # TODO: Verification logic
â”‚   â”œâ”€â”€ agents/            # Agents responsible for key workflows
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search_agent/          # Search agent package
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scrapper_agent/        # Scrapper agent package
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_agent/             # RAG agent package
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ verification_pipeline.py # LangChain RAG pipeline implementation
â”‚   â”œâ”€â”€ scrapers/          # Web scraping module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ web_scraper.py # TODO: Web scraping implementation
â”‚   â””â”€â”€ utils/             # Helper utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py     # TODO: Utility functions
â”œâ”€â”€ tests/                 # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_main.py       # TODO: Test implementations
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ venv/                 # Virtual environment (created by user)
```

## âš™ï¸ Setup & Installation

### Prerequisites

-   Python 3.8 or higher
-   pip (Python package installer)

### Environment Setup

1. **Clone the repository**

    ```bash
    git clone <repository-url>
    cd hive-backend
    ```

2. **Create virtual environment**

    ```bash
    python3 -m venv venv
    ```

3. **Activate virtual environment**

    ```bash
    # On macOS/Linux:
    source venv/bin/activate

    # On Windows:
    venv\Scripts\activate
    ```

4. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

## â–¶ï¸ Running the Server

### Development Mode

```bash
# Make sure virtual environment is activated
uvicorn app.main:app --reload
```

The server will start at `http://localhost:8000`

### API Documentation

-   **Swagger UI**: `http://localhost:8000/docs`
-   **ReDoc**: `http://localhost:8000/redoc`

### Health Check

-   **Health endpoint**: `http://localhost:8000/health`
-   **Configuration test**: `http://localhost:8000/config/test`

## ğŸ”„ Verification Flow

The content verification system follows this high-level flow:

1. **Content Submission**: User submits a post to the platform
2. **Web Scraping**: System scrapes relevant sources for fact-checking
3. **RAG Pipeline**: LangChain processes scraped data and post content
4. **Classification**: AI classifies the post as verified/unverified/misleading
5. **Result Storage**: Verification results are stored and displayed

### Components:

-   **Scrapers**: Extract information from news sites, fact-checking sources
-   **RAG Pipeline**: LangChain-based retrieval and generation for content analysis
-   **Verification Service**: Business logic for processing and storing results

## ğŸš€ Next Steps

### Team Assignments:

**ğŸ§‘â€ğŸ’» Dhruv Pokhriyal**

-   **Primary**: RAG Pipeline (`app/agents/rag_agent/verification_pipeline.py`)
-   **Secondary**: Authentication system and API endpoints (helping Karthik)
-   **Collaboration**: Verification service and utilities

**ğŸ§‘â€ğŸ’» Ankit Sinha**

-   **Primary**: Web Scraping (`app/scrapers/web_scraper.py`)
-   **Secondary**: Verification service coordination
-   **Collaboration**: Integration with RAG pipeline

**ğŸ§‘â€ğŸ’» Karthik**

-   **Primary**: API Endpoints (`app/api/routes.py`)
-   **Secondary**: Post service and business logic
-   **Collaboration**: All team members for integration

### Immediate Tasks:

1. **Implement API Routes** (`app/api/routes.py`) - **ASSIGNED TO: Karthik (with Dhruv's help for auth)**

    - User authentication endpoints
    - Post CRUD operations
    - Community management
    - Verification endpoints

2. **Configure Settings** (`app/core/config.py`) - **ASSIGNED TO: Team collaboration**

    - Database configuration
    - API keys and environment variables
    - Application settings

3. **Create Data Models** (`app/models/schemas.py`) - **ASSIGNED TO: Team collaboration**
    - User, Post, Community schemas
    - Verification result models
    - API request/response models

### Core Features:

4. **Web Scraping** (`app/scrapers/web_scraper.py`) - **ASSIGNED TO: Ankit Sinha**

    - Implement scraping logic for fact-checking sources
    - Handle different content types and sources
    - Error handling and rate limiting

5. **RAG Pipeline** (`app/agents/rag_agent/verification_pipeline.py`) - **ASSIGNED TO: Dhruv Pokhriyal**

    - LangChain document retrieval
    - Question-answering system
    - Content classification logic

6. **Business Logic** (`app/services/`) - **ASSIGNED TO: Team collaboration**
    - Post service implementation
    - Verification service logic
    - User management services

### Testing:

7. **Test Suite** (`tests/`) - **ASSIGNED TO: Team collaboration**
    - Unit tests for all components
    - Integration tests for API endpoints
    - Mock tests for external services

## ğŸ› ï¸ Development

### Code Style

-   Follow PEP 8 guidelines
-   Use type hints where applicable
-   Document functions and classes

### Testing

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=app
```

### Environment Variables

1. **Set up environment (recommended):**

    ```bash
    python setup_env.py
    ```

    **Or manually copy the template:**

    ```bash
    cp env.example .env
    ```

2. **Configure your environment variables in `.env`:**

    ```env
    # Supabase Configuration (Required)
    SUPABASE_URL=https://your-project-id.supabase.co
    SUPABASE_ANON_KEY=your-supabase-anon-key
    SUPABASE_SERVICE_ROLE_KEY=your-supabase-service-role-key

    # OpenAI API (Required for RAG Pipeline)
    OPENAI_API_KEY=your-openai-api-key

    # Security (Required)
    SECRET_KEY=your-secret-key

    # Optional: External APIs
    NEWS_API_KEY=your-news-api-key
    FACT_CHECK_API_KEY=your-fact-check-api-key
    ```

3. **Get your Supabase credentials:**

    - Go to [Supabase Dashboard](https://supabase.com/dashboard)
    - Create a new project or select existing one
    - Go to Settings â†’ API
    - Copy the Project URL and API keys

4. **Get your OpenAI API key:**

    - Go to [OpenAI Platform](https://platform.openai.com/api-keys)
    - Create a new API key
    - Copy the key to your `.env` file

<!-- ## ğŸ“ License

[Add your license information here] -->

<!-- ## ğŸ¤ Contributing

[Add contribution guidelines here] -->
